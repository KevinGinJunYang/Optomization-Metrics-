{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "Learn: execution time=0.044 seconds\n",
      "R2: 0.92\n",
      "MSE: 1324168.94\n",
      "RMSE: 1150.73\n",
      "MAE: 743.13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Learn: execution time=0.112 seconds\n",
      "R2: 0.95\n",
      "MSE: 774686.78\n",
      "RMSE: 880.16\n",
      "MAE: 437.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Learn: execution time=0.035 seconds\n",
      "R2: 0.92\n",
      "MSE: 1324178.2\n",
      "RMSE: 1150.73\n",
      "MAE: 743.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best')\n",
      "Learn: execution time=0.225 seconds\n",
      "R2: 0.97\n",
      "MSE: 529566.17\n",
      "RMSE: 727.71\n",
      "MAE: 352.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Learn: execution time=1.461 seconds\n",
      "R2: 0.98\n",
      "MSE: 333880.54\n",
      "RMSE: 577.82\n",
      "MAE: 285.35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "Learn: execution time=2.512 seconds\n",
      "R2: 0.97\n",
      "MSE: 555862.23\n",
      "RMSE: 745.56\n",
      "MAE: 406.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=1000, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "Learn: execution time=8.196 seconds\n",
      "R2: 0.92\n",
      "MSE: 1326874.68\n",
      "RMSE: 1151.9\n",
      "MAE: 747.29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is an example to perform simple linear regression algorithm on the dataset (weight and height),\n",
    "where x = weight and y = height.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 309\n",
    "# Freeze the random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "train_test_split_test_size = 0.3\n",
    "\n",
    "# Training settings\n",
    "alpha = 0.1  # step size\n",
    "max_iters = 50  # max iterations\n",
    "\n",
    "algorithms = {\n",
    "    \"LR\": LinearRegression(),\n",
    "    \"KNNR\": KNeighborsRegressor(),\n",
    "    \"R\": Ridge(),\n",
    "    \"DTR\": DecisionTreeRegressor(),\n",
    "    \"RFR\": RandomForestRegressor(),\n",
    "    \"GBR\": GradientBoostingRegressor(),\n",
    "    \"SGDR\": SGDRegressor(max_iter = 1000),\n",
    "    \"SVR\": SVR(degree=3, C=100, epsilon=.01),\n",
    "    \"LSVR\": LinearSVR(),\n",
    "    \"MLPR\": MLPRegressor(max_iter = 1000),\n",
    "}\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Data from CSV\n",
    "    :return: df    a panda data frame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"../data/diamonds.csv\")\n",
    "    df = df.drop(columns=df.columns[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_preprocess(data):\n",
    "    \"\"\"\n",
    "    Data preprocess:\n",
    "        1. Split the entire dataset into train and test\n",
    "        2. Split outputs and inputs\n",
    "        3. Standardize train and test\n",
    "        4. Add intercept dummy for computation convenience\n",
    "    :param data: the given dataset (format: panda DataFrame)\n",
    "    :return: train_data       train data contains only inputs\n",
    "             train_labels     train data contains only labels\n",
    "             test_data        test data contains only inputs\n",
    "             test_labels      test data contains only labels\n",
    "             train_data_full       train data (full) contains both inputs and labels\n",
    "             test_data_full       test data (full) contains both inputs and labels\n",
    "    \"\"\"\n",
    "    # Split the data into train and test\n",
    "    train_data, test_data = train_test_split(data, test_size=train_test_split_test_size, random_state=seed)\n",
    "\n",
    "    # Pre-process data (both train and test)\n",
    "    train_data_full = train_data.copy()\n",
    "    train_data = train_data.drop([\"price\"], axis=1)\n",
    "    train_labels = train_data_full[\"price\"]\n",
    "\n",
    "    test_data_full = test_data.copy()\n",
    "    test_data = test_data.drop([\"price\"], axis=1)\n",
    "    test_labels = test_data_full[\"price\"]\n",
    "    #create dummy for catagorical values \n",
    "    train_data = pd.get_dummies(train_data, columns=['cut'])\n",
    "    train_data = pd.get_dummies(train_data, columns=['color'])\n",
    "    train_data = pd.get_dummies(train_data, columns=['clarity'])\n",
    "    test_data = pd.get_dummies(test_data, columns=['cut'])\n",
    "    test_data = pd.get_dummies(test_data, columns=['color'])\n",
    "    test_data = pd.get_dummies(test_data, columns=['clarity'])\n",
    "    # Standardize the inputs\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "\n",
    "    # Tricks: add dummy intercept to both train and test\n",
    "    train_data['intercept_dummy'] = pd.Series(1.0, index=train_data.index)\n",
    "    test_data['intercept_dummy'] = pd.Series(1.0, index=test_data.index)\n",
    "    return train_data, train_labels, test_data, test_labels, train_data_full, test_data_full\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    train_data, train_labels, test_data, test_labels, train_data_full, test_data_full = data_preprocess(data)\n",
    "\n",
    "    # Step 3: Learning Start\n",
    "    for method in algorithms:\n",
    "        clf = algorithms[method]\n",
    "        start_time = datetime.datetime.now()  # Track learning starting time\n",
    "        clf.fit(train_data.values, train_labels.values)\n",
    "        end_time = datetime.datetime.now()  # Track learning ending time\n",
    "        exection_time = (end_time - start_time).total_seconds()  # Track execution time\n",
    "        prediction = clf.predict(test_data.values)\n",
    "\n",
    "        # Step 4: Results presentation\n",
    "        print(clf)\n",
    "        print(\"Learn: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "        # Build baseline model\n",
    "        print(\"R2:\", float(\"{0:.2f}\".format(r2_score(test_labels.values, prediction))))  # R2 should be maximize\n",
    "        print(\"MSE:\", float(\"{0:.2f}\".format(mean_squared_error(test_labels.values, prediction))))\n",
    "        print(\"RMSE:\", float(\"{0:.2f}\".format(np.sqrt(mean_squared_error(test_labels.values, prediction)))))\n",
    "        print(\"MAE:\", float(\"{0:.2f}\".format(mean_absolute_error(test_labels.values, prediction), \"\\n\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
